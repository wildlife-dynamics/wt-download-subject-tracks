# AUTOGENERATED BY ECOSCOPE-WORKFLOWS; see fingerprint in README.md for details
import json
import os

from ecoscope_workflows_core.tasks.config import set_string_var as set_string_var
from ecoscope_workflows_core.tasks.config import (
    set_workflow_details as set_workflow_details,
)
from ecoscope_workflows_core.tasks.filter import (
    get_timezone_from_time_range as get_timezone_from_time_range,
)
from ecoscope_workflows_core.tasks.filter import set_time_range as set_time_range
from ecoscope_workflows_core.tasks.groupby import set_groupers as set_groupers
from ecoscope_workflows_core.tasks.groupby import split_groups as split_groups
from ecoscope_workflows_core.tasks.io import persist_text as persist_text
from ecoscope_workflows_core.tasks.io import set_er_connection as set_er_connection
from ecoscope_workflows_core.tasks.results import (
    create_map_widget_single_view as create_map_widget_single_view,
)
from ecoscope_workflows_core.tasks.results import gather_dashboard as gather_dashboard
from ecoscope_workflows_core.tasks.results import (
    merge_widget_views as merge_widget_views,
)
from ecoscope_workflows_core.tasks.skip import (
    any_dependency_skipped as any_dependency_skipped,
)
from ecoscope_workflows_core.tasks.skip import any_is_empty_df as any_is_empty_df
from ecoscope_workflows_core.tasks.skip import never as never
from ecoscope_workflows_core.tasks.transformation import (
    add_temporal_index as add_temporal_index,
)
from ecoscope_workflows_core.tasks.transformation import (
    convert_values_to_timezone as convert_values_to_timezone,
)
from ecoscope_workflows_core.tasks.transformation import map_columns as map_columns
from ecoscope_workflows_ext_custom.tasks.io import (
    persist_df_wrapper as persist_df_wrapper,
)
from ecoscope_workflows_ext_custom.tasks.skip import maybe_skip_df as maybe_skip_df
from ecoscope_workflows_ext_custom.tasks.transformation import (
    apply_sql_query as apply_sql_query,
)
from ecoscope_workflows_ext_custom.tasks.transformation import (
    drop_column_prefix as drop_column_prefix,
)
from ecoscope_workflows_ext_ecoscope.tasks.io import (
    get_subjectgroup_observations as get_subjectgroup_observations,
)
from ecoscope_workflows_ext_ecoscope.tasks.preprocessing import (
    relocations_to_trajectory as relocations_to_trajectory,
)
from ecoscope_workflows_ext_ecoscope.tasks.results import (
    create_polyline_layer as create_polyline_layer,
)
from ecoscope_workflows_ext_ecoscope.tasks.results import draw_ecomap as draw_ecomap
from ecoscope_workflows_ext_ecoscope.tasks.results import set_base_maps as set_base_maps
from ecoscope_workflows_ext_ecoscope.tasks.skip import (
    all_geometry_are_none as all_geometry_are_none,
)
from ecoscope_workflows_ext_ecoscope.tasks.transformation import (
    apply_color_map as apply_color_map,
)
from ecoscope_workflows_ext_ecoscope.tasks.transformation import (
    apply_reloc_coord_filter as apply_reloc_coord_filter,
)
from ecoscope_workflows_ext_ecoscope.tasks.warning import (
    mixed_subtype_warning as mixed_subtype_warning,
)

from ..params import Params


def main(params: Params):
    params_dict = json.loads(params.model_dump_json(exclude_unset=True))

    workflow_details = (
        set_workflow_details.validate()
        .set_task_instance_id("workflow_details")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(**(params_dict.get("workflow_details") or {}))
        .call()
    )

    time_range = (
        set_time_range.validate()
        .set_task_instance_id("time_range")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            time_format="%d %b %Y %H:%M:%S", **(params_dict.get("time_range") or {})
        )
        .call()
    )

    get_timezone = (
        get_timezone_from_time_range.validate()
        .set_task_instance_id("get_timezone")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(time_range=time_range, **(params_dict.get("get_timezone") or {}))
        .call()
    )

    er_client_name = (
        set_er_connection.validate()
        .set_task_instance_id("er_client_name")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(**(params_dict.get("er_client_name") or {}))
        .call()
    )

    subject_obs = (
        get_subjectgroup_observations.validate()
        .set_task_instance_id("subject_obs")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            client=er_client_name,
            time_range=time_range,
            raise_on_empty=False,
            **(params_dict.get("subject_obs") or {}),
        )
        .call()
    )

    warn_if_mixed_subtype = (
        mixed_subtype_warning.validate()
        .set_task_instance_id("warn_if_mixed_subtype")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                never,
            ],
            unpack_depth=1,
        )
        .partial(
            subject_obs=subject_obs, **(params_dict.get("warn_if_mixed_subtype") or {})
        )
        .call()
    )

    groupers = (
        set_groupers.validate()
        .set_task_instance_id("groupers")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(**(params_dict.get("groupers") or {}))
        .call()
    )

    obs_add_temporal_index = (
        add_temporal_index.validate()
        .set_task_instance_id("obs_add_temporal_index")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            df=subject_obs,
            time_col="fixtime",
            groupers=groupers,
            cast_to_datetime=True,
            format="mixed",
            **(params_dict.get("obs_add_temporal_index") or {}),
        )
        .call()
    )

    split_obs_groups = (
        split_groups.validate()
        .set_task_instance_id("split_obs_groups")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            df=obs_add_temporal_index,
            groupers=groupers,
            **(params_dict.get("split_obs_groups") or {}),
        )
        .call()
    )

    convert_to_user_timezone = (
        convert_values_to_timezone.validate()
        .set_task_instance_id("convert_to_user_timezone")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            timezone=get_timezone,
            columns=["fixtime"],
            **(params_dict.get("convert_to_user_timezone") or {}),
        )
        .mapvalues(argnames=["df"], argvalues=split_obs_groups)
    )

    drop_extra_prefix = (
        drop_column_prefix.validate()
        .set_task_instance_id("drop_extra_prefix")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            prefix="extra__",
            duplicate_strategy="suffix",
            **(params_dict.get("drop_extra_prefix") or {}),
        )
        .mapvalues(argnames=["df"], argvalues=convert_to_user_timezone)
    )

    filter_obs = (
        apply_reloc_coord_filter.validate()
        .set_task_instance_id("filter_obs")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            roi_gdf=None,
            roi_name=None,
            reset_index=False,
            **(params_dict.get("filter_obs") or {}),
        )
        .mapvalues(argnames=["df"], argvalues=drop_extra_prefix)
    )

    customize_columns_obs = (
        map_columns.validate()
        .set_task_instance_id("customize_columns_obs")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(**(params_dict.get("customize_columns_obs") or {}))
        .mapvalues(argnames=["df"], argvalues=filter_obs)
    )

    sql_query_obs = (
        apply_sql_query.validate()
        .set_task_instance_id("sql_query_obs")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(**(params_dict.get("sql_query_obs") or {}))
        .mapvalues(argnames=["df"], argvalues=customize_columns_obs)
    )

    persist_obs = (
        persist_df_wrapper.validate()
        .set_task_instance_id("persist_obs")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                never,
            ],
            unpack_depth=1,
        )
        .partial(
            root_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            sanitize=True,
            **(params_dict.get("persist_obs") or {}),
        )
        .mapvalues(argnames=["df"], argvalues=sql_query_obs)
    )

    subject_traj = (
        relocations_to_trajectory.validate()
        .set_task_instance_id("subject_traj")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(**(params_dict.get("subject_traj") or {}))
        .mapvalues(argnames=["relocations"], argvalues=sql_query_obs)
    )

    drop_extra_prefix_traj = (
        drop_column_prefix.validate()
        .set_task_instance_id("drop_extra_prefix_traj")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            prefix="extra__",
            duplicate_strategy="suffix",
            **(params_dict.get("drop_extra_prefix_traj") or {}),
        )
        .mapvalues(argnames=["df"], argvalues=subject_traj)
    )

    customize_columns_internally = (
        map_columns.validate()
        .set_task_instance_id("customize_columns_internally")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            rename_columns={},
            drop_columns=["id"],
            retain_columns=[],
            **(params_dict.get("customize_columns_internally") or {}),
        )
        .mapvalues(argnames=["df"], argvalues=drop_extra_prefix_traj)
    )

    customize_columns = (
        map_columns.validate()
        .set_task_instance_id("customize_columns")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(**(params_dict.get("customize_columns") or {}))
        .mapvalues(argnames=["df"], argvalues=customize_columns_internally)
    )

    sql_query = (
        apply_sql_query.validate()
        .set_task_instance_id("sql_query")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(**(params_dict.get("sql_query") or {}))
        .mapvalues(argnames=["df"], argvalues=customize_columns)
    )

    traj_add_temporal_index = (
        add_temporal_index.validate()
        .set_task_instance_id("traj_add_temporal_index")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            time_col="segment_start",
            groupers=groupers,
            cast_to_datetime=True,
            format="mixed",
            **(params_dict.get("traj_add_temporal_index") or {}),
        )
        .mapvalues(argnames=["df"], argvalues=sql_query)
    )

    persist_tracks = (
        persist_df_wrapper.validate()
        .set_task_instance_id("persist_tracks")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                never,
            ],
            unpack_depth=1,
        )
        .partial(
            root_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            sanitize=True,
            **(params_dict.get("persist_tracks") or {}),
        )
        .mapvalues(argnames=["df"], argvalues=traj_add_temporal_index)
    )

    skip_map_generation = (
        maybe_skip_df.validate()
        .set_task_instance_id("skip_map_generation")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(**(params_dict.get("skip_map_generation") or {}))
        .mapvalues(argnames=["df"], argvalues=traj_add_temporal_index)
    )

    set_traj_map_title = (
        set_string_var.validate()
        .set_task_instance_id("set_traj_map_title")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            var="Subject Group Trajectory Map",
            **(params_dict.get("set_traj_map_title") or {}),
        )
        .call()
    )

    base_map_defs = (
        set_base_maps.validate()
        .set_task_instance_id("base_map_defs")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(**(params_dict.get("base_map_defs") or {}))
        .call()
    )

    colormap_traj = (
        apply_color_map.validate()
        .set_task_instance_id("colormap_traj")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            input_column_name="subject__name",
            output_column_name="subject__name_colormap",
            colormap="tab20b",
            **(params_dict.get("colormap_traj") or {}),
        )
        .mapvalues(argnames=["df"], argvalues=skip_map_generation)
    )

    rename_display_columns = (
        map_columns.validate()
        .set_task_instance_id("rename_display_columns")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            drop_columns=[],
            retain_columns=[],
            rename_columns={
                "segment_start": "Start",
                "timespan_seconds": "Duration (s)",
                "speed_kmhr": "Speed (kph)",
                "subject__name": "Subject Name",
                "subject__sex": "Subject Sex",
            },
            **(params_dict.get("rename_display_columns") or {}),
        )
        .mapvalues(argnames=["df"], argvalues=colormap_traj)
    )

    traj_map_layers = (
        create_polyline_layer.validate()
        .set_task_instance_id("traj_map_layers")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
                all_geometry_are_none,
            ],
            unpack_depth=1,
        )
        .partial(
            layer_style={"color_column": "subject__name_colormap"},
            legend={
                "label_column": "Subject Name",
                "color_column": "subject__name_colormap",
            },
            tooltip_columns=[
                "Start",
                "Duration (s)",
                "Speed (kph)",
                "Nighttime",
                "Subject Name",
                "Subject Sex",
            ],
            **(params_dict.get("traj_map_layers") or {}),
        )
        .mapvalues(argnames=["geodataframe"], argvalues=rename_display_columns)
    )

    traj_ecomap = (
        draw_ecomap.validate()
        .set_task_instance_id("traj_ecomap")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            tile_layers=base_map_defs,
            north_arrow_style={"placement": "top-left"},
            legend_style={
                "title": "Subject",
                "format_title": False,
                "placement": "bottom-right",
            },
            static=False,
            title=None,
            max_zoom=20,
            widget_id=set_traj_map_title,
            **(params_dict.get("traj_ecomap") or {}),
        )
        .mapvalues(argnames=["geo_layers"], argvalues=traj_map_layers)
    )

    ecomap_html_urls = (
        persist_text.validate()
        .set_task_instance_id("ecomap_html_urls")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            root_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            filename_suffix="v2",
            **(params_dict.get("ecomap_html_urls") or {}),
        )
        .mapvalues(argnames=["text"], argvalues=traj_ecomap)
    )

    traj_map_widgets_single_views = (
        create_map_widget_single_view.validate()
        .set_task_instance_id("traj_map_widgets_single_views")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                never,
            ],
            unpack_depth=1,
        )
        .partial(
            title=set_traj_map_title,
            **(params_dict.get("traj_map_widgets_single_views") or {}),
        )
        .map(argnames=["view", "data"], argvalues=ecomap_html_urls)
    )

    traj_grouped_map_widget = (
        merge_widget_views.validate()
        .set_task_instance_id("traj_grouped_map_widget")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            widgets=traj_map_widgets_single_views,
            **(params_dict.get("traj_grouped_map_widget") or {}),
        )
        .call()
    )

    subject_tracking_dashboard = (
        gather_dashboard.validate()
        .set_task_instance_id("subject_tracking_dashboard")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            details=workflow_details,
            widgets=[traj_grouped_map_widget],
            groupers=groupers,
            time_range=time_range,
            warning=warn_if_mixed_subtype,
            **(params_dict.get("subject_tracking_dashboard") or {}),
        )
        .call()
    )

    return subject_tracking_dashboard
